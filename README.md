# ğŸ­ VR Companion â€” Interactive Scene Composer with ComfyUI & LLMs

**VR Companion** is an interactive, AI-driven character system combining real-time conversation, affective reasoning, and image generation via **ComfyUI**. The companion reacts to your chat by composing structured edit prompts that drive image-to-image updatesâ€”great for VR or browser-based storytelling.

---

## ğŸ§  Core Loop

```
User input â†’ LLM interpretation â†’ Structured edit plan â†’ ComfyUI render â†’ Live image refresh
```

- Short, in-character dialogue.
- Behind the scenes: explicit, sectioned edit instructions that guide the **Nunchaku Qwen Image Edit** workflow.

---

## âš™ï¸ Architecture

| Layer | Purpose |
|------|---------|
| **ComfyUI (Nunchaku Qwen Edit workflow)** | Diffusion-based image editing from structured prompts. |
| **Python orchestration** (`companion_web_v10x.py`) | Gradio UI + LLM planner + ComfyUI runner. |
| **Gradio web UI** | Chat, live image panel, â€œcomposed promptâ€ preview, heartbeat-driven refresh. |
| **Scene Composer** | Interprets intent â†’ fills slots: Background, Face, Body, Hands/Props, **Wardrobe**, Camera/Light, Style. |
| **LLM backends** | OpenAI GPTâ€‘4oâ€‘mini or local Ollama models for dialogue + planning. |

---

## ğŸŒŸ Key Features

- **Structured Edit Prompting** (slot-based):  
  Clear sections (BACKGROUND / FACE / BODY / HANDS / **WARDROBE** / CAMERA+LIGHT / STYLE) that diffusion edit models follow reliably.
- **Sceneâ€“Character Fusion**:  
  Merges environment, gesture, mood, and optional backstory into a single, controllable prompt.
- **Continuity & Real-Time UX**:  
  Heartbeat polling keeps the last frame visible while rendering the next; choose Base / Last render / Custom as the next input.
- **Creative Controls**:  
  Per-aspect toggles and intensity sliders (0â€“2), â€œvariationâ€ for stronger background changes, live â€œComposed Promptâ€ preview + history log.
- **Multi-Backend Dialogue**:  
  Pluggable OpenAI or **Ollama**; JSON-structured planning for reproducibility.

---

## ğŸ§° Requirements

- Python **3.11+**
- Local **ComfyUI**
- Python libs: `gradio`, `requests`, `pillow`
- Optional: **OpenAI** API key or **Ollama** runtime for LLM responses

Quick install:
```bash
pip install gradio requests pillow
```

---

## ğŸš€ Run

1. Start **ComfyUI** locally (default REST: `http://127.0.0.1:8188`).
2. Ensure your workflow JSON for **Nunchaku Qwen Edit Lightning** is reachable (see UI field).
3. Launch the web app:
   ```bash
   python companion_web_v10e.py
   ```
4. Open the local Gradio URL (e.g., `http://127.0.0.1:7860`) and try:
   > â€œLetâ€™s go to a gothic cathedral, candle in hand.â€

Youâ€™ll see a short, in-character reply and a detailed **composed prompt** that updates the image.

---

## ğŸ”§ Configuration (in the UI)

- **Input source**: Base image / Last render / Custom; optional upload of last render back to Comfyâ€™s input.  
- **Scene Composer**: Creative boost, style preset (cinema/portrait/moody/fantasy), backstory, baseline mood, variation.  
- **Structured Edit Mode**:  
  - Per-slot toggles (Background, Body, Face, Hands, **Wardrobe**, Camera/Light, Style).  
  - Intensity sliders per aspect (0â€“2).  
  - Editable template with placeholders like `{{BACKGROUND}}`, `{{FACE}}`, etc.  
- **Planner & Gating**: Change-score Î” (0â€“4) vs last plan, threshold + cooldown, **Manual render** button.  
- **LLM**: OpenAI / Ollama / None; Ping button and visible status.

---

## ğŸ§  Technical Highlights

- Asynchronous Gradio chain: **Prime â†’ Plan â†’ Render** with chat state and no flicker.  
- **Heartbeat polling** of `./outputs` to keep last frame on-screen until the new image lands.  
- Hybrid **heuristics + LLM** for environment/gesture/mood/wardrobe inference.  
- Deterministic, transparent planning via **JSON** messages and a visible prompt preview.  
- Direct subprocess call to your `nunchaku_edit.py` driver (minimal coupling, no REST JSON templating required).

---

## ğŸª„ Example (Structured Edit Mode)

```
EDIT INSTRUCTIONS (clear, actionable):
BACKGROUND â†’ gothic cathedral interior, stained glass windows, dust rays
FACE â†’ soft smile, gaze right
BODY / GESTURE â†’ relaxed posture, hands together
HANDS / PROPS â†’ hold a small candle
WARDROBE â†’ dark wool coat with subtle embroidery
CAMERA / LIGHT â†’ close-up, cinematic lighting, volumetric rays
STYLE â†’ cinematic lighting, filmic contrast
```

---

## ğŸ§© Roadmap

- Emotion + memory state machine for persistent personality.  
- VR front-end (Aâ€‘Frame/WebXR) and Meta Quest testing.  
- Direct ComfyUI queue integration + streaming progress.  
- Wardrobe presets & pose libraries.

---

## ğŸ™Œ Credits

Built by **Stephen Hicks** as part of ongoing research into **AIâ€‘mediated emotional presence**, **neural storytelling**, and **interactive cinematic worlds**.

---

## ğŸ“„ License

MIT â€” see `LICENSE`.

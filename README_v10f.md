
# Companion Web v10f

**Companion Web** is an interactive AI-driven VR companion and visual storytelling interface.  
Version 10f integrates **OpenAI GPTâ€‘4oâ€‘mini** as the default conversational model and introduces stronger **head, face, and gesture control** for dynamic character animation through **ComfyUI** workflows.

---

## ğŸŒŸ Features

### Conversational AI
- Default backend: **OpenAI GPTâ€‘4oâ€‘mini** (requires `OPENAI_API_KEY`).
- Optional local backend: **Ollama** (e.g. `llama3.1:8b`).
- Inâ€‘character conversation with emotion, gestures, and scene awareness.
- Autoâ€‘generation of structured visual prompts describing **pose, gaze, hands, wardrobe, and environment**.

### Enhanced Character Motion
- New expressivity sliders:
  - **Head movement** â†’ look left/right/up/down, gentle tilt.
  - **Expression variance** â†’ microâ€‘expressions and emotional shifts.
  - **Gesture energy** â†’ talkingâ€‘hands emphasis and openâ€‘palm dynamics.
- More vivid **HEAD / GAZE** and **HANDS / GESTURES / PROPS** sections in prompt composition.
- Automatically adjusts **camera framing** and **lighting** for cinematic continuity.

### Memory System
- YAMLâ€‘based longâ€‘term memory (perâ€‘character).
- Stores transcripts, gestures, episodes, and pinned facts.
- Automatically saved and reloaded on app restart.
- Manual â€œSave Memoryâ€, â€œPin Factâ€, and â€œSummarize Sessionâ€ buttons.

### Scene Composition & Rendering
- Generates structured edit instructions for **ComfyUI** via the `nunchaku_edit.py` workflow.
- Choice of image input source: Base, Last Render, or Custom.
- Automatic gating for render triggering (thresholdâ€¯+â€¯cooldown).
- Optional â€œManual Renderâ€ override button.

### Album Integration
- Save current frame and metadata (â€œSave to Albumâ€).
- Thumbnail gallery of previous renders with prompt captions.
- Memory entries record album saves as episodes.

### Live Preview & Heartbeat
- Image autoâ€‘refreshes every second, maintaining continuity.
- Prevents blank screen between renders when ComfyUI output updates slowly.

---

## ğŸ§  Dependencies

| Component | Purpose |
|------------|----------|
| **Gradio** | Web UI and reactive state management |
| **Requests** | REST communication with OpenAI and ComfyUI |
| **PyYAML** | Persistent character memory storage |
| **ComfyUI** | Image generation workflow engine |
| **memory_manager.py** | Handles episodic / semantic memory |
| **album_manager.py** | Image gallery and metadata persistence |
| **nunchaku_edit.py** | Bridge script for ComfyUI edit workflows |

Optional: **Ollama** for local LLM execution.

Install with:
```bash
pip install gradio requests pyyaml pillow
```

---

## ğŸš€ Usage

1. Ensure your **ComfyUI** server and **nunchaku_edit.py** workflow are running.
2. Export your OpenAI key:
   ```bash
   export OPENAI_API_KEY=sk-...
   ```
3. Launch the companion:
   ```bash
   python companion_web_v10f.py
   ```
4. Open your browser at `http://127.0.0.1:7860`.
5. Chat with your character and watch her respond in dialogue and pose.

---

## ğŸ§© Project Structure

```
/companion_web_v10f.py      # Main app (Gradio frontend + LLM + ComfyUI orchestration)
/nunchaku_edit.py           # ComfyUI workflow bridge
/memory_manager.py          # Persistent character memories
/album_manager.py           # Local image album manager
/memory/                    # Character memory YAMLs
/outputs/                   # Rendered image outputs
/input/                     # Base reference images
```

---

## ğŸ—‚ï¸ Version Highlights

**v10f (current)**
- Default OpenAI backend (gptâ€‘4oâ€‘mini).
- Dramatically improved head, expression, and gesture dynamics.
- Added wardrobe and prop awareness.
- Optimized gating & heartbeat behaviour.
- Integrated album and memory persistence.

**v10e and earlier**
- Introduced memory persistence and structured edit templates.
- Added album saving, session summaries, and gating thresholds.

---

## ğŸ“„ License

This project is released under the **MIT License**.

---

## ğŸ’¬ Acknowledgements

Developed collaboratively with iterative AI feedback by **Stephen Hicks** (Mendea / Fusion Shift projects).  
Inspired by cinematic embodiment, humanâ€“AI coâ€‘presence, and VR storytelling research.
